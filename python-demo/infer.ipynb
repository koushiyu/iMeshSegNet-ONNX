{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vedo\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
    "    return idx\n",
    "\n",
    "def get_graph_feature(x, k=20, idx=None, dim9=False, device='cpu'):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    if idx is None:\n",
    "        if dim9 == False:\n",
    "            idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "        else:\n",
    "            idx = knn(x[:, 6:], k=k)\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "    idx = idx + idx_base\n",
    "    idx = idx.view(-1)\n",
    "    _, num_dims, _ = x.size()\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centring(mesh: vedo.Mesh):\n",
    "    mesh.points(pts=mesh.points()-mesh.centerOfMass())\n",
    "    return mesh\n",
    "\n",
    "def get_metadata(model_name: str, mesh: vedo.Mesh, device='cpu'):\n",
    "    # mesh = centring(mesh)\n",
    "    N = mesh.NCells()\n",
    "    points = vedo.vtk2numpy(mesh.polydata().GetPoints().GetData())\n",
    "    ids = vedo.vtk2numpy(mesh.polydata().GetPolys().GetData()).reshape((N, -1))[:,1:]\n",
    "    cells = points[ids].reshape(N, 9).astype(dtype='float32')\n",
    "    normals = vedo.vedo2trimesh(mesh).face_normals\n",
    "    normals.setflags(write=1)\n",
    "    barycenters = mesh.cellCenters()\n",
    "    \n",
    "    #normalized data\n",
    "    maxs = points.max(axis=0)\n",
    "    mins = points.min(axis=0)\n",
    "    means = points.mean(axis=0)\n",
    "    stds = points.std(axis=0)\n",
    "    nmeans = normals.mean(axis=0)\n",
    "    nstds = normals.std(axis=0)\n",
    "\n",
    "    for i in range(3):\n",
    "        cells[:, i] = (cells[:, i] - means[i]) / stds[i] #point 1\n",
    "        cells[:, i+3] = (cells[:, i+3] - means[i]) / stds[i] #point 2\n",
    "        cells[:, i+6] = (cells[:, i+6] - means[i]) / stds[i] #point 3\n",
    "        barycenters[:,i] = (barycenters[:,i] - mins[i]) / (maxs[i]-mins[i])\n",
    "        normals[:,i] = (normals[:,i] - nmeans[i]) / nstds[i]\n",
    "\n",
    "    X = np.column_stack((cells, barycenters, normals))\n",
    "    X = X.transpose(1, 0)\n",
    "\n",
    "    meta = dict()\n",
    "    meta[\"input\"] = torch.from_numpy(X).unsqueeze(0).to(device, dtype=torch.float)\n",
    "\n",
    "    if model_name == \"iMeshSegNet\":\n",
    "        print(\"Getting KG6 and KG12.\")\n",
    "        KG_6 = get_graph_feature(torch.from_numpy(X[9:12, :]).unsqueeze(0), k=6).squeeze(0)\n",
    "        KG_12 = get_graph_feature(torch.from_numpy(X[9:12, :]).unsqueeze(0), k=12).squeeze(0)\n",
    "        meta[\"KG_6\"] = KG_6.unsqueeze(0).to(device, dtype=torch.float)\n",
    "        meta[\"KG_12\"] = KG_12.unsqueeze(0).to(device, dtype=torch.float)\n",
    "    elif model_name == \"MeshSegNet\":\n",
    "        print(\"Getting A_S and A_L.\")\n",
    "        X = X.transpose(1, 0)\n",
    "        # computing A_S and A_L\n",
    "        A_S = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n",
    "        A_L = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n",
    "        TX = torch.as_tensor(X[:, 9:12], device='cpu')\n",
    "        TD = torch.cdist(TX, TX)\n",
    "        D = TD.cpu().numpy()\n",
    "        # D = distance_matrix(X[:, 9:12], X[:, 9:12])\n",
    "        A_S[D<0.1] = 1.0\n",
    "        A_S = A_S / np.dot(np.sum(A_S, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n",
    "\n",
    "        A_L[D<0.2] = 1.0\n",
    "        A_L = A_L / np.dot(np.sum(A_L, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n",
    "\n",
    "        # numpy -> torch.tensor\n",
    "        A_S = A_S.reshape([1, A_S.shape[0], A_S.shape[1]])\n",
    "        A_L = A_L.reshape([1, A_L.shape[0], A_L.shape[1]])\n",
    "        meta[\"A_S\"] = torch.from_numpy(A_S).to(device, dtype=torch.float)\n",
    "        meta[\"A_L\"] = torch.from_numpy(A_L).to(device, dtype=torch.float)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "def allocate_buffers(engine, batch):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for item in zip(engine, batch):\n",
    "        binding, sub = item\n",
    "        size = trt.volume(sub.shape)\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]\n",
    "\n",
    "def do_inference_v2(context, bindings, inputs, outputs, stream):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]\n",
    "\n",
    "def load_np_to_input_buffer(item, pagelocked_buffer):\n",
    "    np.copyto(pagelocked_buffer, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_path = \"/home/ziyang/Desktop/iMeshSegNet-ONNX/onnx/model_sim.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning ONNX file parsing\n",
      "[09/05/2022-11:57:41] [TRT] [W] onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "Completed parsing of ONNX file\n",
      "[09/05/2022-11:57:41] [TRT] [W] TensorRT was linked against cuDNN 8.4.1 but loaded cuDNN 8.3.2\n",
      "[09/05/2022-11:58:00] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[09/05/2022-11:58:00] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n"
     ]
    }
   ],
   "source": [
    "TRT_LOGGER = trt.Logger()\n",
    "\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(common.EXPLICIT_BATCH)\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "runtime = trt.Runtime(TRT_LOGGER)\n",
    "\n",
    "# Parse model file\n",
    "with open(onnx_file_path, \"rb\") as model:\n",
    "    print(\"Beginning ONNX file parsing\")\n",
    "    if not parser.parse(model.read()):\n",
    "        print(\"ERROR: Failed to parse the ONNX file.\")\n",
    "        for error in range(parser.num_errors):\n",
    "            print(parser.get_error(error))\n",
    "print(\"Completed parsing of ONNX file\")\n",
    "\n",
    "opt_profile = builder.create_optimization_profile()\n",
    "opt_profile.set_shape(input='input', min=(1,15,1000), opt=(2,15,5000), max=(2,15,10000))\n",
    "opt_profile.set_shape(input='a_s', min=(1,1000,1000), opt=(2,5000,5000), max=(2,10000,10000))\n",
    "opt_profile.set_shape(input='a_l', min=(1,1000,1000), opt=(2,5000,5000), max=(2,10000,10000))\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "\n",
    "config.add_optimization_profile(opt_profile)\n",
    "\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30) # 256MiB*2*2*2*2\n",
    "\n",
    "plan = builder.build_serialized_network(network, config)\n",
    "\n",
    "engine = runtime.deserialize_cuda_engine(plan)\n",
    "\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting A_S and A_L.\n",
      "torch.Size([1, 15, 5000])\n",
      "Infer time:  0.10907363891601562\n",
      "Total time:  0.8495852947235107\n",
      "1.0\n",
      "[8.49054632e-05 5.96667496e-06 4.25081089e-06 4.15956549e-08\n",
      " 5.01430577e-08 1.41140597e-04 8.87975679e-04 1.21823945e-07\n",
      " 1.01396523e-07 1.46261455e-05 3.64778716e-05 1.35645436e-07\n",
      " 1.09602911e-06 9.80355963e-03 9.89018559e-01 1.03043874e-06\n",
      " 3.19302771e-08]\n",
      "(1, 5000, 17)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "\n",
    "small_batch = get_metadata(\"MeshSegNet\", vedo.Mesh(\"/home/ziyang/Desktop/iMeshSegNet-ONNX/mesh/test_1.ply\"), \"cpu\")\n",
    "print(small_batch[\"input\"].shape)\n",
    "batch_size = small_batch[\"input\"].shape[0]\n",
    "points_num = small_batch[\"input\"].shape[2]\n",
    "\n",
    "context.set_binding_shape(0, (batch_size, 15, points_num))\n",
    "context.set_binding_shape(1, (batch_size, points_num, points_num))\n",
    "context.set_binding_shape(2, (batch_size, points_num, points_num))\n",
    "\n",
    "output = np.empty((batch_size, points_num, 17), dtype=np.float32)\n",
    "small_batch = (small_batch[\"input\"].numpy(), small_batch['A_S'].numpy(), small_batch['A_L'].numpy(), output)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "inputs, outputs, bindings, stream = allocate_buffers(engine, small_batch)\n",
    "\n",
    "for idx, item in enumerate(small_batch[:-1]):\n",
    "    load_np_to_input_buffer(item.ravel(), pagelocked_buffer=inputs[idx].host)\n",
    "\n",
    "output = do_inference_v2(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "print('Infer time: ', time.time()-t2)\n",
    "\n",
    "out = np.asarray(output[0]).reshape(batch_size,points_num,17)\n",
    "print('Total time: ', time.time()-t1)\n",
    "\n",
    "print(out.max())\n",
    "print(out[0][4999])\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1, 15, -1)\n",
      "(-1, -1, -1)\n",
      "(-1, -1, -1)\n",
      "(-1, -1, 17)\n",
      "(1, 15, 5000)\n",
      "(1, 5000, 5000)\n",
      "(1, 5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(engine.get_binding_shape(0))\n",
    "print(engine.get_binding_shape(1))\n",
    "print(engine.get_binding_shape(2))\n",
    "print(engine.get_binding_shape(3))\n",
    "print(context.get_binding_shape(0))\n",
    "print(context.get_binding_shape(1))\n",
    "print(context.get_binding_shape(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bindings = [int(d_input), int(d_a_s), int(d_a_l), int(d_output)]\n",
    "\n",
    "# print(bindings)\n",
    "\n",
    "# context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)\n",
    "\n",
    "# cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "\n",
    "# stream.synchronize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('TensorRT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1df3cb460791b8235d0208473b025ee3b9870df92738d4b37a88f6d87a477e87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
